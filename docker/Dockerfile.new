# apex
RUN if [ "$ENABLE_TIGHT_BUILD_RES" = "2" ]; then \
          export NVCC_APPEND_FLAGS="--threads 1"; \
        else \
          export NVCC_APPEND_FLAGS="--threads 4"; \
        fi && \
  pip -v install --disable-pip-version-check --no-cache-dir \
  --no-build-isolation \
  --config-settings "--build-option=--cpp_ext --cuda_ext --parallel 8" git+https://github.com/NVIDIA/apex.git

# transformer engine, we install with --no-deps to avoid installing torch and torch-extensions
RUN pip install pybind11

# flash attn
# the newest version megatron supports is v2.7.4
RUN if [ "$ENABLE_TIGHT_BUILD_RES" = "2" ]; then \
      export MAX_JOBS=4; \
    elif [ "$ENABLE_TIGHT_BUILD_RES" = "1" ]; then \
      export MAX_JOBS=32; \
    else \
      export MAX_JOBS=64; \
    fi && \
    pip -v install flash-attn==2.7.4.post1 --no-build-isolation

RUN pip install flash-linear-attention

# TE does not have wheel on cuda 13 yet, thus need to install from source
RUN if [ "${ENABLE_CUDA_13}" = "1" ]; then \
          pip -v install --no-build-isolation git+https://github.com/NVIDIA/TransformerEngine.git@stable; \
        else \
          pip -v install --no-build-isolation "transformer_engine[pytorch]"; \
        fi

RUN git clone https://github.com/Dao-AILab/flash-attention.git && \
    cd flash-attention/ && git submodule update --init && cd hopper/ && python setup.py install && \
    export python_path=`python -c "import site; print(site.getsitepackages()[0])"` && \
    mkdir -p $python_path/flash_attn_3 && \
    cp flash_attn_interface.py $python_path/flash_attn_3/flash_attn_interface.py

WORKDIR /root/
RUN git clone https://github.com/NVIDIA/Megatron-LM.git --recursive && \
    cd Megatron-LM && git checkout ${MEGATRON_COMMIT} && \
    pip install -e .

# sandwitch norm for GLM models
COPY patch/${SGLANG_VERSION}/megatron.patch /root/Megatron-LM/
RUN cd Megatron-LM && \
    git update-index --refresh && \
    git apply megatron.patch --3way && \
    if grep -R -n '^<<<<<<< ' .; then \
      echo "Patch failed to apply cleanly. Please resolve conflicts." && \
      exit 1; \
    fi && \
    rm megatron.patch

# TODO
## sglang patch
#COPY patch/${SGLANG_VERSION}/sglang.patch /sgl-workspace/sglang/
#RUN cd /sgl-workspace/sglang && \
#  git update-index --refresh && \
#  git apply sglang.patch && \
#  if grep -R -n '^<<<<<<< ' .; then \
#    echo "Patch failed to apply cleanly. Please resolve conflicts." && \
#    exit 1; \
#  fi && \
#  rm sglang.patch

RUN rm /root/.tmux.conf

# This patch from masahi will be included in later Triton releases
RUN (cd /root && git clone -b feat/v350_plus_8045 https://github.com/fzyzcjy/triton.git && cd triton && pip install -r python/requirements.txt && pip install --verbose -e .)

# This patch is merged into main, but we are using stable version, thus still need it
RUN curl -L https://github.com/NVIDIA/TransformerEngine/pull/2286.patch -o /root/te2286.patch && (cd /usr/local/lib/python3.12/dist-packages/transformer_engine && (patch -p2 < /root/te2286.patch))

# temporarily hack fix issue in GB300 base image
RUN SGL_KERNEL_VERSION=0.3.16.post4 && \
    python3 -m pip install https://github.com/sgl-project/whl/releases/download/v${SGL_KERNEL_VERSION}/sgl_kernel-${SGL_KERNEL_VERSION}+cu130-cp310-abi3-manylinux2014_$(uname -m).whl --force-reinstall --no-deps

# temporary hack
# NOTE: must patch DeepEP *before* reinstalling it
RUN curl -L https://github.com/fzyzcjy/DeepEP/commit/814e508537c6ffc775d59f6f1b9ba43f3a65968c.patch -o /root/temp.patch && (cd /sgl-workspace/DeepEP && (patch -p1 < /root/temp.patch))

# temporary hack
RUN rm -rf /sgl-workspace/nvshmem && \
    unset NVSHMEM_DIR && \
    (cd /sgl-workspace/DeepEP && TORCH_CUDA_ARCH_LIST="9.0;10.0;10.3" pip install --no-build-isolation --verbose .)

# temporary hack
RUN SGL_KERNEL_VERSION=0.3.17 && python3 -m pip install https://github.com/sgl-project/whl/releases/download/v${SGL_KERNEL_VERSION}/sgl_kernel-${SGL_KERNEL_VERSION}+cu130-cp310-abi3-manylinux2014_$(uname -m).whl --force-reinstall --no-deps

# temporary hack
RUN curl -L https://github.com/NVIDIA/Megatron-LM/commit/d8c6aa4c0b5d4c15ec1196802bce292d4580ed4a.patch -o /root/temp.patch && (cd /root/Megatron-LM && (patch -p1 < /root/temp.patch))
